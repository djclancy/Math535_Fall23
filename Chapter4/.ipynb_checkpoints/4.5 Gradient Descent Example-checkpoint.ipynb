{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d9e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a80db",
   "metadata": {},
   "source": [
    "Let's load the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d228e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(imgs, labels), (test_imgs, test_labels) = mnist.load_data()\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53f5ea",
   "metadata": {},
   "source": [
    "Here are the binary examples where the labels are either $0$ or $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83601e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "i01 = [i for i in range(len(labels)) if (labels[i]==0) or (labels[i]==1)]\n",
    "imgs01 = imgs[i01]\n",
    "labels01 = labels[i01]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69248a41",
   "metadata": {},
   "source": [
    "Our goal is to find a function $\\hat{f}:\\mathbb{R}^d\\to \\mathbb{R}$ such that for our examples $\\{(\\mathbf{x}_i,y_i): i = 1,\\dotsm,n\\}$ such that $\\hat{f}(\\mathbf{x}_i)\\approx y_i$ for all $i$.\n",
    "\n",
    "This is a problem of binary classification (we are only restricting to $0$ and $1$s) there is a natural way that we can approach this **supervised learning** problem. (It is **supervised** because we know the correct labels).\n",
    "\n",
    "This will seem similar to the linear regression problem:\n",
    "\n",
    "1) We define a class of functions $\\widehat{\\mathcal{F}}$ (a collection of classifiers)\n",
    "\n",
    "2) We define a loss function $\\ell(\\hat{f},(\\mathbf{x},y))$ which quantifies how close $\\hat{f}(\\mathbf{x})$ is to $y$.\n",
    "\n",
    "We then want to find a function $\\hat{f}$ which minimizes the average loss:\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\ell\\left(\\hat{f},(\\mathbf{x}_i,y_i)\\right).\n",
    "$$\n",
    "\n",
    "One example is *logistic regression* where we consider the classifiers of the form\n",
    "$$\n",
    "\\hat{f}(\\mathbf{x}) = \\sigma\\left(\\mathbf{x}^T\\boldsymbol{\\theta}\\right) \\qquad \\sigma(t) = \\frac{1}{1+e^{-t}}\n",
    "$$\n",
    "where $\\boldsymbol{\\theta}\\in\\mathbb R^d$ is a parameter.\n",
    "\n",
    "Note that $\\sigma(5) \\approx 6.7\\times 10^{-3}$ and $\\sigma(-5) \\approx 0.993$.\n",
    "\n",
    "We then use the cross-entropy loss (common with logistic regression)\n",
    "$$\n",
    "\\ell\\left(\\hat{f},(\\mathbf{x},y)\\right) = -y\\log(\\sigma(\\mathbf{x}^T\\boldsymbol{\\theta})) - (1-y) \\log(1-\\sigma(\\mathbf{x}^T\\boldsymbol{\\theta})).\n",
    "$$\n",
    "\n",
    "We then want to minimize\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\theta}\\in\\mathbb R^d} - \\sum_{i=1}^n y_i\\log\\left(\\sigma\\left(\\mathbf{x}_i^T\\boldsymbol{\\theta}\\right)\\right) + (1-y) \\log\\left(1-\\sigma\\left(\\mathbf{x}_i^T\\boldsymbol{\\theta}\\right)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db7169",
   "metadata": {},
   "source": [
    "We now want to minimize this function:\n",
    "$$\n",
    "F(\\boldsymbol{\\theta}) = -\\frac{1}{n}\\sum_{i=1}^n\\left( y_i \\log (\\sigma(\\mathbf{x}_i^T\\boldsymbol{\\theta})) + (1-y_i) \\log (1-\\sigma(\\mathbf{x}_i^T\\boldsymbol{\\theta}))\\right)\n",
    "$$so let's look at its gradient. First note that \n",
    "$$\n",
    "\\sigma'(t) = \\frac{e^{-t}}{(1+e^{-t})^2} = \\frac{1}{1+e^{-t}} \\frac{e^{-t}}{1+e^{-t}} = \\sigma(t) (\\sigma(t)-1).\n",
    "$$ Therefore\n",
    "$$\n",
    "\\frac{d}{dt} \\log(\\sigma(t)) = \\frac{1}{\\sigma(t)} \\sigma'(t) = \\sigma(t)-1\n",
    "$$\n",
    "Similarly\n",
    "$$\n",
    "\\frac{d}{dt} \\log(1-\\sigma(t)) = \\frac{1}{1-\\sigma(t)} (-\\sigma'(t)) = \\sigma(t).\n",
    "$$\n",
    "\n",
    "So\n",
    "$$\n",
    "\\frac{d}{dt} \\left( y \\log (\\sigma(t)) + (1-y)\\log(1-\\sigma(t))\\right) = y(\\sigma(t)-1) + (1-y)\\sigma(t) = \\sigma(t)-y.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94758c",
   "metadata": {},
   "source": [
    "We can now compute the gradient of $F$ using the chain rule:\n",
    "$$\n",
    "\\nabla F (\\boldsymbol{\\theta}) = \\frac{1}{n}\\sum_{p=1}^n \\left(y_p - \\sigma(\\mathbf{x}_p^T\\boldsymbol{\\theta})  \\right)\\mathbf{x}_p\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125a390",
   "metadata": {},
   "source": [
    "Let's look the $i$th entry of this gradient:\n",
    "$$\n",
    "\\partial_{\\theta_i} F =\\frac{1}{n} \\sum_{p=1}^n (\\mathbf{x}_p)_i (y_p - \\sigma(\\mathbf{x}_p^T\\boldsymbol{\\theta})) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e27eb",
   "metadata": {},
   "source": [
    "We can then see that\n",
    "$$\n",
    "\\partial_{\\theta_j}\\partial_{\\theta_i} F= \\frac{1}{n} \\sum_{p=1}^n (\\mathbf{x}_p)_i (-\\sigma'(\\mathbf{x}_p^T \\boldsymbol{\\theta})) (\\mathbf{x}_p)_j = \\frac{1}{n}\\sum_{p=1}^n \\sigma(\\mathbf{x}_p^T \\boldsymbol{\\theta})(1-\\sigma(\\mathbf{x}_p^T \\boldsymbol{\\theta})) (\\mathbf{x}_p)_i (\\mathbf{x}_p)_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0ab23",
   "metadata": {},
   "source": [
    "But $\\mathbf{v}_i \\mathbf{u}_j$ is the $ij$th entry in the matrix $\\mathbf{v} \\mathbf{u}^T$ and so we can write the above as a matrix via\n",
    "$$\n",
    "H_F(\\boldsymbol{\\theta}) = \\frac{1}{n}\\sum_{p=1}^n \\sigma(\\mathbf{x}_p^T \\boldsymbol{\\theta})(1-\\sigma(\\mathbf{x}_p^T \\boldsymbol{\\theta})) \\mathbf{x}_p \\mathbf{x}_p^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c5f60527",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([imgs01[i].flatten() for i in range(len(labels01))])\n",
    "y = labels01\n",
    "X = X[:100]\n",
    "y = y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "38cfbfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nozeroSpots = []\n",
    "for j in range(len(X[0,:])):\n",
    "    if np.sum(X[:,j])!=0:\n",
    "        nozeroSpots.append(j)\n",
    "X = X[:,nozeroSpots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8b9e0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(100)\n",
    "param = [1/1000 for j in range(np.shape(X)[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1df9b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    return np.double(1/(1+np.exp(-t)))\n",
    "def grad(X,y,param):\n",
    "    N = len(y)\n",
    "    gr = np.zeros(len(X[0,:]))\n",
    "    for j in range(N):\n",
    "        gr = gr + N**(-1)*(y[j]-sigma(np.matmul(X[j,:],param)))*X[j,:]\n",
    "    return gr\n",
    "def loss(X,y,param):\n",
    "    lval = 0\n",
    "    N = len(y)\n",
    "    for j in range(N):\n",
    "        if y[j] == 0:\n",
    "            lval = lval - ((1-y[j])*np.log(1-sigma(np.matmul(X[j,:],param))))\n",
    "        else: \n",
    "            lval = lval - y[j]*np.log(sigma(np.matmul(X[j,:],param)))\n",
    "    return lval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab675c5d",
   "metadata": {},
   "source": [
    "We can now try to implement gradient descent. By a lemma in the notes, we can bound the Hessian's largest eigenvalue by\n",
    "$$\n",
    "\\lambda_1(H_F(\\boldsymbol{\\theta})) \\le \\frac{1}{n}\\sum_{i=1}^n \\|\\mathbf{x}\\|_2^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fa566d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "L = np.linalg.norm(X)**2/N\n",
    "iteration = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2bad2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iteration):\n",
    "    param = param-100*grad(X,y,param)/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "340c5d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "1 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "0 1.0\n",
      "1 1.0\n",
      "1 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(N):\n",
    "    print(y[i] , sigma(np.matmul(X[i,:],param)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaee7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074c863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a1ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
