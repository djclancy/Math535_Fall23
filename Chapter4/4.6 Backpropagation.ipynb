{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b91e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90590c2",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Suppose that we have data $(\\mathbf{x}_i,y_i)_{1\\le i\\le n}$ where $\\mathbf{x}\\in\\mathbb{R}^d$ and $y_i\\in \\mathbb{R}$ and we want do regression by finding a good function $f:\\mathbb{R}^d\\to\\mathbb{R}$ that fits this data well:\n",
    "$$\n",
    "\\sum_{i=1}^n \\ell(f(\\mathbf{x}_i),my_i)\\text{ is small},\n",
    "$$\n",
    "where $C$ is some cost function.\n",
    "\n",
    "We have seen two main ways to do this but both involve parametrizing the collection of functions $f$ so that they are really functions of some parameter $\\boldsymbol{\\theta}\\in \\mathbb{R}^p$ so that we really want to minimize\n",
    "$$\n",
    "\\arg \\min_{\\boldsymbol{\\theta}}\\sum_{i=1}^n \\ell\\left(f(\\mathbf{x}_i,\\boldsymbol{\\theta}),y_i\\right).\n",
    "$$\n",
    "\n",
    "The examples that we've looked at have had comuptable by-hand (although perhaps ugly) expressions for their gradients and Hessians where we've been able to use convex analysis, and in particular gradient descent, to minimize the cost. \n",
    "\n",
    "What happens when things get more complicated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d844f",
   "metadata": {},
   "source": [
    "Many important current-day models for regression and classification use multi-layer progressive functions. We'll stick with a classification problem here.\n",
    "\n",
    "Our data is $(\\mathbf{x}, y)$ has $\\mathbf{x}\\in \\mathbb{R}^d$ and $y = \\{label_1,label_2,\\dotsm, _label_K\\}$ is assigned one of $K$ many labels. In practice it is often easier to redefine the discrete options for $y$ into a continuous variable by saying that label $i\\in\\{1,\\dotsm,K\\}$ is actually the standard basis vector in $\\mathbf{R}^K$. This transforms our data $y\\in \\{label_1,\\dotsm,label_K\\}$ to $\\mathbf{y} \\in \\Delta^K := \\{(p_1,\\dotsm,p_K): \\sum_{i=1}^K p_i = 1, p_i\\ge 0\\}.$\n",
    "\n",
    "What this means is that we have start with data $\\mathbf{z}_1 = \\mathbf{x}\\in \\mathbb{R}^d$. This is our input for *layer 1*. We write $n_1 = d$.\n",
    "\n",
    "We then apply a function $\\mathbf{g}_1:\\mathbb{R}^{n_1+ r_1} \\to \\mathbb{R}^{n_2}$ which takes our initial data $\\mathbf{z}_1\\in \\mathbb{R}^{n_1}$ and some parameter vecotr $\\mathbf{w}_1\\in \\mathbb{R}^{r_1}$ and gives us new data in $\\mathbb{R}^{n_2}$. Formally\n",
    "$$\n",
    "\\mathbf{z}_2 = \\mathbf{g}_1 (\\mathbf{z}_1, \\mathbf{w}_1).\n",
    "$$\n",
    "\n",
    "We then take this transformed data and transform it again and again. We have functions\n",
    "$$\n",
    "\\mathbf{g}_i:\\mathbb{R}^{n_i+r_i}\\to \\mathbb{R}^{n_{i+1}},\\qquad i = 1,2,\\dotsm, L\n",
    "$$\n",
    "that takes an input $\\mathbf{z}_i\\in \\mathbb{R}^{n_i}$ and some parameters $\\mathbf{w}_i\\in \\mathbb{R}^{r_i}$ and gives us an output $\\mathbf{z}_{i+1}\\in \\mathbb{R}^{n_{i+1}}$ that we feed into the next layer. \n",
    "\n",
    "At the end we have some function\n",
    "$$\n",
    "\\mathbf{h}:\\mathbb{R}^{d+ r_1+r_2+\\dotsm+r_L}\\to \\mathbb{R}^{n_{L+1}},\\qquad (n_{L+1} = K)\n",
    "$$\n",
    "defined by\n",
    "$$\n",
    "\\mathbf{h}(\\mathbf{x}, \\overline{\\mathbf{w}}) = \\mathbf{g}_L\\Bigg(\\mathbf{g}_{L-1}\\bigg(\\dotsm, \\mathbf{g}_2\\Big(\\mathbf{g}_1(\\mathbf{x},\\mathbf{w}_1), \\mathbf{w}_2\\Big),\\dotsm),\\mathbf{w}_{L-1}\\bigg) ,\\mathbf{w}_L  \\Bigg).\n",
    "$$\n",
    "\n",
    "Finally, we want to minimize a cost $\\ell( \\mathbf{h}(\\mathbf{x},\\overline{\\mathbf{w}}),\\mathbf{y})$ or the empirical loss\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n \\ell(\\mathbf{h}(\\mathbf{x}_i,\\overline{\\mathbf{w}}),\\mathbf{y}_i)\n",
    "$$\n",
    "\n",
    "**BUT HOW ON EARTH DO WE COMPUTE THE OPTIMAL PARAMETER!!!?!?!??!!?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7cff0f",
   "metadata": {},
   "source": [
    "## Derivatives of vector valued functions\n",
    "\n",
    "We first start with how to differentiate vector valued functions: $\\mathbf{f}:\\mathbb{R}^d\\to \\mathbb{R}^m$ is made up of $m$ many coordinate functions\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}) = \\begin{bmatrix}\n",
    "f_1(\\mathbf{x})\\\\\n",
    "f_2(\\mathbf{x})\\\\\n",
    "\\vdots\\\\\n",
    "f_m (\\mathbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We know that when we have just a single function $f:\\mathbb{R}\\to \\mathbb{R}$ then \n",
    "$$\n",
    "f(x+h) \\approx f(x) + f'(x) h\n",
    "$$\n",
    "and\n",
    "when we have $f:\\mathbb{R}^d\\to \\mathbb{R}$ then\n",
    "$$\n",
    "f(\\mathbf{x}+ h\\mathbf{v}) \\approx f(\\mathbf{x}) + \\frac{\\partial f}{\\partial \\mathbf{v}}(\\mathbf{x})h = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T \\mathbf{v} h\n",
    "$$\n",
    "or using solely vectors\n",
    "$$\n",
    "f(\\mathbf{x}+\\mathbf{h}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T \\mathbf{h}.\n",
    "$$ \n",
    "**Both give linear approximations.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8af539",
   "metadata": {},
   "source": [
    "We extend this to vector valued functions. \n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}+\\mathbf{h}) \\approx \\mathbf{f}(\\mathbf{x}) + T( \\mathbf{h})\n",
    "$$\n",
    "where $T$ is a linear approximation that sends the vector $\\mathbf{h}$ to a new vector.\n",
    "\n",
    "First we've added $\\mathbf{h}$ to $\\mathbf{x}\\in \\mathbb{R}^d$ and we need that to make sense! That means we need $T$ to take in vectors in $\\mathbf{R}^{d}$. Also we've added $T(\\mathbf{h})$ to $\\mathbf{f}(\\mathbf{x})$ which lives in $\\mathbb{R}^m$ and we need that addition to make sense which forces $T(\\mathbf{h})\\in \\mathbb{R}^m$.  \n",
    "\n",
    "So the derivative of $\\mathbf{f}$ at $\\mathbf{x}$ ought to be a linear function which takes a vector in $\\mathbb{R}^d$ and spits out a vector in $\\mathbb{R}^m$. That means $T(\\mathbf{h}) = A\\mathbf{h}$ where $A\\in \\mathbb{R}^{m\\times d}.$\n",
    "\n",
    "Now let's make a definition and state a lemma.\n",
    "\n",
    "**Definition:** We say that a function $\\mathbf{f}:\\mathbb{R}^d\\to \\mathbf{R}^m$ is **differentiable** at $\\mathbf{x}$ if there is a matrix $A\\in \\mathbb{R}^{m\\times d}$ such that\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}+\\mathbf{h}) =\\mathbf{ f}(\\mathbf{x}) + A\\mathbf{h} + \\mathbf{r}(\\mathbf{h})\n",
    "$$\n",
    "for some remainder term $\\mathbf{r}$ such that\n",
    "$$\n",
    "\\lim_{\\mathbf{h}\\to \\mathbf{0}} \\frac{\\|\\mathbf{r}(\\mathbf{h})\\|}{\\|\\mathbf{h}\\|} = 0.\n",
    "$$\n",
    "(Both the matrix $A$ and the function $\\mathbf{r}$ can depend on what $\\mathbf{x}$ we look at.)\n",
    "\n",
    "We call the (abstractly defined) matrix $A$ above the **differential** of $\\mathbf{f}$ at $\\mathbf{x}$.\n",
    "\n",
    "**Theorem/Definition** Suppose that $\\mathbf{f}:\\mathbb{R}^d\\to \\mathbb{R}^m$ is a function and $\\mathbf{x}_0\\in \\mathbb{R}^d$. Suppose that $\\frac{\\partial f_j}{\\partial x_i}(\\mathbf{x}_0)$ exists for all $i = 1,\\dots,d$ and $j = 1,\\dots, m$ and the partial derivatives are continuous at $\\mathbf{x}_0$. Then the differential $A$ exists, is unique, and is equal to the **Jacobian** of $\\mathbf{f}$ at $\\mathbf{x}_0$ defined by\n",
    "$$\n",
    "J_{\\mathbf{f}} (\\mathbf{x}_0) = \\begin{bmatrix}\\displaystyle\n",
    "\\frac{\\partial f_1}{\\partial x_1}(\\mathbf{x_0})&\\displaystyle\\frac{\\partial f_1}{\\partial x_2}(\\mathbf{x_0})&\\dotsm&\\displaystyle\\frac{\\partial f_1}{\\partial x_d}(\\mathbf{x_0})\\\\\n",
    "\\displaystyle\\frac{\\partial f_2}{\\partial x_1}(\\mathbf{x_0})&\\displaystyle\\frac{\\partial f_2}{\\partial x_2}(\\mathbf{x_0})&\\dotsm&\\displaystyle\\frac{\\partial f_2}{\\partial x_d}(\\mathbf{x_0})\\\\\n",
    "\\vdots&\\vdots&&\\vdots\\\\\n",
    "\\displaystyle\\frac{\\partial f_m}{\\partial x_1}(\\mathbf{x_0})&\\displaystyle\\frac{\\partial f_m}{\\partial x_2}(\\mathbf{x_0})&\\displaystyle\\dotsm&\\frac{\\partial f_m}{\\partial x_d}(\\mathbf{x_0})\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Note\n",
    "$$\n",
    "J_{\\mathbf{f}}(\\mathbf{x}_0) = \\begin{bmatrix}\n",
    "\\displaystyle \\nabla f_1(\\mathbf{x}_0)^T\\\\\n",
    "\\displaystyle \\nabla f_2(\\mathbf{x}_0)^T\\\\\n",
    "\\dotsm\\\\\n",
    "\\displaystyle \\nabla f_d(\\mathbf{x}_0)^T\\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bfc057",
   "metadata": {},
   "source": [
    "**Example** If $f:\\mathbb{R}^d\\to \\mathbf{R}$ then its Jacobian is \n",
    "$$\n",
    "J_f(\\mathbf{x}) = \\nabla f(\\mathbf{x})^T.\n",
    "$$\n",
    "\n",
    "**Example** If $\\mathbf{f}:\\mathbb{R}^d\\to \\mathbf{R}^m$ is the affine map\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}) = A\\mathbf{x}+\\mathbf{b}\n",
    "$$\n",
    "then its Jacobian is \n",
    "$$\n",
    "J_{\\mathbf{f}}(\\mathbf{x}) = A.\n",
    "$$ Why? The matrix $A$ is the differential:\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}+\\mathbf{h}) = A(\\mathbf{x}+\\mathbf{h}) + \\mathbf{b} = A\\mathbf{x}+\\mathbf{b} + A\\mathbf{h} = f(\\mathbf{x})+A\\mathbf{h} + \\mathbf{0}\n",
    "$$\n",
    "where the zero at the end is the remainder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9241eb",
   "metadata": {},
   "source": [
    "Given two functions $f:X\\to Y$ and $g:Y\\to Z$ we write $g\\circ f: X\\to Z$ for $g(f(x))$. This is the **composition** of $f$ and $g$ and we say that $g$ is composed with $f$.\n",
    "\n",
    "**Theorem (Chain Rule):** Let $\\mathbf{f}:\\mathbb{R}^d\\to \\mathbb{R}^m$ and $\\mathbf{g}:\\mathbb{R}^m\\to \\mathbb{R}^p$. Suppose that $\\mathbf{f}$ is continuously differentiable at $\\mathbf{x}_0$ and $\\mathbf{g}$ is continously differentiable at $\\mathbf{f}(\\mathbf{x}_0)$. Then $\\mathbf{g}\\circ\\mathbf{f}$ is continuously differentiable at $\\mathbf{x}_0$ and its Jacobian \n",
    "$$\n",
    "J_{\\mathbf{g}\\circ \\mathbf{f}} (\\mathbf{x}_0) =  J_{\\mathbf{g}}(\\mathbf{f}(\\mathbf{x}_0)) J_{\\mathbf{f}}(\\mathbf{x_0}).\n",
    "$$\n",
    "\n",
    "*Proof:* By the theorem above, the Jacobian is the differential. Therefore we compute the differential. We write $A$ for the differential of $\\mathbf{f}$ and $B$ for the differential of $\\mathbf{g}$.\n",
    "\\begin{align*}\n",
    "\\mathbf{g}\\left(\\mathbf{f}(\\mathbf{x}+\\mathbf{h}) \\right) &=  \\mathbf{g}\\Big( \\mathbf{f}(\\mathbf{x}_0)+ A \\mathbf{h} + \\mathbf{r}_1(\\mathbf{h})\\Big)\\\\\n",
    "&= \\mathbf{g}(\\mathbf{f}(\\mathbf{x}_0)) + B (A \\mathbf{h} + \\mathbf{r}_1(\\mathbf{h})) + \\mathbf{r}_2(A\\mathbf{h}+\\mathbf{r}_1(\\mathbf{h}))\\\\\n",
    "&= \\mathbf{g}\\circ \\mathbf{f}(\\mathbf{x}_0) + B A \\mathbf{h} +\\mathbf{r}_3(\\mathbf{h}),\\\\\n",
    "\\mathbf{r}_3(\\mathbf{h})&=A\\mathbf{r}_1(\\mathbf{h}) + \\mathbf{r}_2(A\\mathbf{h}+\\mathbf{r}_1(\\mathbf{h})).\n",
    "\\end{align*}\n",
    "It is a little tedious to verify, but remainder satisfies\n",
    "$$\n",
    "\\lim_{\\mathbf{h}\\to\\mathbf{0}} \\frac{\\|\\mathbf{r}_3(\\mathbf{h})\\|}{\\|\\mathbf{h}\\|} = 0.\n",
    "$$ (You should try to do this on your own!)\n",
    "\n",
    "Since $BA =  J_{\\mathbf{g}}(\\mathbf{f}(\\mathbf{x}_0)) J_{\\mathbf{f}}(\\mathbf{x_0})$ is the differential of $\\mathbf{g}\\circ\\mathbf{f}$, the Jacobain statement follows. $\\square$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba8ec1",
   "metadata": {},
   "source": [
    "## Back to backpropagation\n",
    "\n",
    "Recall that we have a \n",
    "$$\\mathbf{h}(\\mathbf{x}, \\overline{\\mathbf{w}}) = \\mathbf{g}_L\\Bigg(\\mathbf{g}_{L-1}\\bigg(\\dotsm, \\mathbf{g}_2\\Big(\\mathbf{g}_1(\\mathbf{x},\\mathbf{w}_1), \\mathbf{w}_2\\Big),\\dotsm),\\mathbf{w}_{L-1}\\bigg) ,\\mathbf{w}_L  \\Bigg)$$\n",
    "and we want to minimize \n",
    "$$\n",
    "\\ell( \\mathbf{h}(\\mathbf{x},\\overline{\\mathbf{w}}),\\mathbf{y}).$$\n",
    "\n",
    "\n",
    "First since we are treating $\\mathbf{x}$ and $\\mathbf{y}$ as data that we are given, we never differentiate with respect to them. So we'll treat them as constants and ignore them in our notation of $\\mathbf{h}$ and $\\ell$.\n",
    "\n",
    "$$\\mathbf{h}(\\overline{\\mathbf{w}}) = \\mathbf{g}_L\\Bigg(\\mathbf{g}_{L-1}\\bigg(\\dotsm, \\mathbf{g}_2\\Big(\\mathbf{g}_1(\\mathbf{x},\\mathbf{w}_1), \\mathbf{w}_2\\Big),\\dotsm),\\mathbf{w}_{L-1}\\bigg) ,\\mathbf{w}_L  \\Bigg)$$\n",
    "and we want to minimize a loss $\\ell(\\mathbf{h}(\\overline{\\mathbf{w}}))$. \n",
    "\n",
    "We write\n",
    "$$\n",
    "f(\\overline{\\mathbf{w}}) = \\ell(\\mathbf{h}(\\overline{\\mathbf{w}}))\n",
    "$$\n",
    "\n",
    "Here we think of \n",
    "$$\n",
    "\\overline{\\mathbf{w}} \\in \\mathbb{R}^{r_1+r_2+\\dotsm+r_L}\n",
    "$$\n",
    "is a vector whose first $r_1$ coordinates are $\\mathbf{w}_1$, whose second $r_2$ many coordinates are $\\mathbf{w}_2$ and so on.\n",
    "\n",
    "### Towards some compositions\n",
    "Unfortunately the function $\\mathbf{h}$ does not look like a composition right now. So how do we make this look like compositions? The idea is to view\n",
    "$$\n",
    "\\mathbf{g}_i(\\text{Layer }i\\text{ input}) = \\text{Layer }i\\text{ output}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\text{Layer }i \\text{ input} = \\left(\\text{Layer }i-1 \\text{ output}, \\mathbf{w}_i\\right)\n",
    "$$\n",
    "\n",
    "The input to layer $i$ is some vector $$\\mathcal{I}_i(\\mathbf{w}_1,\\dotsm,\\mathbf{w}_{i-1},\\mathbf{w}_i) = \\mathcal{I}_i(\\overline{\\mathbf{w}}^{i}).$$ And the output at layer $i$ is some vector\n",
    "$$\n",
    "\\mathcal{O}_i(\\overline{\\mathbf{w}}^i) = \\mathbf{g}_i(\\mathcal{I}_i(\\overline{\\mathbf{w}}^i)).\n",
    "$$ Now\n",
    "$$\n",
    "\\mathcal{I}_i(\\overline{\\mathbf{w}}^i ) = \\Big(\\mathcal{O}_{i-1}(\\overline{\\mathbf{w}}^{i-1}), \\mathbf{w}_i\\Big).\n",
    "$$\n",
    "\n",
    "### Jacobians of $\\mathcal{I}$ \n",
    "So the Jacobian of the input has the very nice blockstructure\n",
    "$$\n",
    "J_{\\mathcal{I}_i}(\\overline{\\mathbf{w}}^i) = \\begin{bmatrix}\\displaystyle\n",
    "J_{\\mathcal{O}_{i-1}}(\\overline{\\mathbf{w}}^{i-1}) & \\displaystyle0_{n_{i}\\times r_i}\\\\\n",
    "\\displaystyle0_{r_i\\times n_i} &\\displaystyle I_{r_i\\times r_i}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and since the input to the first layer is $\\mathcal{I}_1 (\\mathbf{w}_1) = \\begin{bmatrix} \\mathbf{x} \\\\\\mathbf{w}_1\\end{bmatrix}$ its Jacobian is\n",
    "$$\n",
    "J_{\\mathcal{I}_1}(\\mathbf{w}_1) = \\begin{bmatrix}\n",
    "0_{d\\times r_1}\\\\\n",
    "I_{r_1\\times r_1}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f54a1a",
   "metadata": {},
   "source": [
    "### Jacobians of $\\mathbf{g}$\n",
    "\n",
    "Let's look at what the Jacobian of a function $\\mathbf{g}(\\mathbf{z},\\mathbf{w})$ looks like:\n",
    "$$\n",
    "J_{\\mathbf{g}}(\\mathbf{z}_0,\\mathbf{w}_0) = \\begin{bmatrix}\n",
    "\\displaystyle \\frac{\\partial g_{1}}{\\partial z_1}(\\mathbf{z}_0,\\mathbf{w}_0)&\\dotsm &\\displaystyle \\frac{\\partial g_{1}}{\\partial z_n}(\\mathbf{z}_0,\\mathbf{w}_0)& \\displaystyle \\frac{\\partial g_{1}}{\\partial w_1}(\\mathbf{z}_0,\\mathbf{w}_0)&\\dotsm&\\displaystyle \\frac{\\partial g_{1}}{\\partial w_r}(\\mathbf{z}_0,\\mathbf{w}_0)\\\\\n",
    "\\displaystyle \\frac{\\partial g_{2}}{\\partial z_1}(\\mathbf{z}_0,\\mathbf{w}_0)&\\dotsm &\\displaystyle \\frac{\\partial g_{2}}{\\partial z_n}(\\mathbf{z}_0,\\mathbf{w}_0)& \\displaystyle \\frac{\\partial g_{2}}{\\partial w_1}(\\mathbf{z}_0,\\mathbf{w}_0)&\\dotsm&\\displaystyle \\frac{\\partial g_{1}}{\\partial w_r}(\\mathbf{z}_0,\\mathbf{w}_0)\\\\\n",
    "\\vdots&&\\vdots&\\vdots&&\\vdots\\\\\n",
    "\\displaystyle \\frac{\\partial g_{m}}{\\partial z_1}(\\mathbf{z}_0,\\mathbf{w}_0)&\\dotsm &\\displaystyle \\frac{\\partial g_{1}}{\\partial z_n}(\\mathbf{z}_0,\\mathbf{w}_0)& \\displaystyle \\frac{\\partial g_{m}}{\\partial w_1}(\\mathbf{z}_0,\\mathbf{w}_0)&\\dotsm&\\displaystyle \\frac{\\partial g_{m}}{\\partial w_r}(\\mathbf{z}_0,\\mathbf{w}_0)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "so we can write this in a \"simple\" block form as \n",
    "$$\n",
    "J_{\\mathbf{g}}(\\mathbf{z},\\mathbf{w}) = \\begin{bmatrix} A & B\\end{bmatrix}\n",
    "$$ where the $A$ term carries all the information about the partial derivatives of ${z}$'s and $B$ carries all the information about the partial derivatives of $w$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273a20d",
   "metadata": {},
   "source": [
    "Recall that the input to layer $i$ is $$\\mathcal{I}_i(\\overline{\\mathbf{w}}^i) = (\\mathcal{O}_{i-1} (\\overline{\\mathbf{w}}^{i-1}),\\mathbf{w}_i) = (\\mathbf{z}_i, \\mathbf{w}_i).$$\n",
    "\n",
    "So we write \n",
    "$$\n",
    "J_{\\mathbf{g}_i}( \\mathbf{z}_i, \\mathbf{w}_i) = \\begin{bmatrix}\n",
    "A_i & B_i\n",
    "\\end{bmatrix}\n",
    "$$ where the matrix $A\\in \\mathbb{R}^{n_{i+1}\\times n_{i}}$ and $B\\in \\mathbb{R}^{n_{i+1}\\times r_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e0183",
   "metadata": {},
   "source": [
    "### Jacobians of Early Layers\n",
    "\n",
    "What is the Jacobian of the first layer? Recall that $A_1\\in \\mathbb{R}^{n_2\\times n_1} = \\mathbb{R}^{n_2\\times d}$ and $B_1 \\in \\mathbb{R}^{n_2\\times r_1}.$\n",
    "$$\n",
    "J_{\\mathcal{O}_1}(\\mathbf{w}_1) = J_{\\mathbf{g}_1}(\\mathbf{x},\\mathbf{w}_1) J_{\\mathcal{I}_1}(\\mathbf{x},\\mathbf{w}_1) = \\begin{bmatrix} A_1 & B_1 \\end{bmatrix} \\begin{bmatrix} 0_{d\\times r_1}\\\\I_{r_1\\times r_1} \\end{bmatrix} = A_1 0_{d\\times r_1} + B_1\\times I_{r_1\\times r_1} = B_1.\n",
    "$$\n",
    "\n",
    "What's the Jacobian of the output of the second layer?\n",
    "\\begin{align*}\n",
    "J_{\\mathcal{O}_2}(\\overline{\\mathbf{w}}^2) &= J_{\\mathbf{g}_2}(\\mathcal{I}_2(\\overline{\\mathbf{w}}^2)) J_{\\mathcal{I}_2}(\\overline{\\mathbf{w}}^2)\\\\\n",
    "&=J_{\\mathbf{g}_2}(\\mathbf{z}_2,\\mathbf{w}_2) J_{\\mathcal{I}_2}(\\overline{\\mathbf{w}}^2)\\\\\n",
    "&=\\begin{bmatrix} A_2 & B_2 \\end{bmatrix} \\begin{bmatrix} \n",
    "J_{\\mathcal{O}_1}(\\overline{\\mathbf{w}}^1) & 0\\\\\n",
    "0 & I_{r_2\\times r_2}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "A_2 J_{\\mathcal{O}_1}(\\overline{\\mathbf{w}}^1) + B_2 0  & A_10+B_2 I_{r_2\\times r_2} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix} A_2 J_{\\mathcal{O}_1}(\\overline{\\mathbf{w}}^1) & B_2\\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "What about the general step?\n",
    "\\begin{align*}\n",
    "J_{\\mathcal{O}_i}(\\overline{\\mathbf{w}}^i) &= J_{\\mathbf{g}_i}(\\mathcal{I}_i(\\overline{\\mathbf{w}}^i)) J_{\\mathcal{I}_i}(\\overline{\\mathbf{w}}^i)\\\\\n",
    "&=J_{\\mathbf{g}_i}(\\mathbf{z}_i,\\mathbf{w}_i) J_{\\mathcal{I}_i}(\\overline{\\mathbf{w}}^i)\\\\\n",
    "&=\\begin{bmatrix} A_i & B_i \\end{bmatrix} \\begin{bmatrix} \n",
    "J_{\\mathcal{O}_{i-1}}(\\overline{\\mathbf{w}}^{i-1}) & 0\\\\\n",
    "0 & I_{r_i\\times r_i}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "A_i J_{\\mathcal{O}_{i-1}}(\\overline{\\mathbf{w}}^{i-1}) + B_i 0  & A_i0+B_i I_{r_i\\times r_i} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix} A_i J_{\\mathcal{O}_{i-1}}(\\overline{\\mathbf{w}}^{i-1}) & B_i\\end{bmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18b1cb",
   "metadata": {},
   "source": [
    "### Gradient of the Cost:\n",
    "\n",
    "What is the gradient of the cost $f(\\overline{\\mathbf{w}})$? \n",
    "\n",
    "$$\n",
    "J_f(\\overline{\\mathbf{w}}) = \\ell(\\mathbf{h}(\\overline{\\mathbf{w}})) = J_{\\ell}(\\mathbf{h}(\\overline{\\mathbf{w}})) J_{\\mathbf{h}}(\\overline{\\mathbf{w}}) = \\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)^T J_{\\mathcal{O}_{L}}(\\overline{\\mathbf{w}}^L).\n",
    "$$\n",
    "But since $\\nabla f = (J_f)^T$, we have\n",
    "\n",
    "$$\n",
    "\\nabla f(\\overline{\\mathbf{w}}) = J_{\\mathcal{O}_{L}}(\\overline{\\mathbf{w}}^L)^T \\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d5a05",
   "metadata": {},
   "source": [
    "### Putting things together:\n",
    "\n",
    "We now know all the formulas to figure out what the gradient of the cost function actually is!\n",
    "\n",
    "Note that $$J_{\\mathcal{O}_i}(\\overline{\\mathbf{w}}^i)^T =  \\begin{bmatrix} A_i J_{\\mathcal{O}_{i-1}}(\\overline{\\mathbf{w}}^{i-1}) & B_i\\end{bmatrix}^T =  \n",
    "\\begin{bmatrix}\n",
    "J_{\\mathcal{O}_{i-1}}(\\overline{\\mathbf{w}}^{i-1})^T A_i^T\\\\\n",
    "B_i^T\n",
    "\\end{bmatrix}\n",
    "\\qquad i = 2,\\dotsm, L$$\n",
    "and\n",
    "$$\n",
    "J_{\\mathcal{O}_1}(\\overline{\\mathbf{w}}^1)^T = B_1^T.$$\n",
    "\n",
    "So\n",
    "\\begin{align*}\n",
    "\\nabla f(\\mathbf{x})&= J_{\\mathcal{O}_{L}}(\\overline{\\mathbf{w}}^L)^T \\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "&= \\begin{bmatrix}\n",
    "J_{\\mathcal{O}_{L-1}}(\\overline{\\mathbf{w}}^{L-1})^T A_L^T\\\\\n",
    "B_L^T\n",
    "\\end{bmatrix} \\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "&=\\begin{bmatrix}\n",
    "J_{\\mathcal{O}_{L-1}}(\\overline{\\mathbf{w}}^{L-1})^T A_L^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "B_L^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\n",
    "\\end{bmatrix} \\\\\n",
    "&=\\begin{bmatrix}\n",
    "\\displaystyle \\begin{bmatrix}\n",
    "J_{\\mathcal{O}_{L-2}}(\\overline{\\mathbf{w}}^{L-2})^T A_{L-1}^T\\\\\n",
    "B_{L-1}^T\n",
    "\\end{bmatrix}A_{L}^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "B_L^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\n",
    "\\end{bmatrix}\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\displaystyle \n",
    "J_{\\mathcal{O}_{L-2}}(\\overline{\\mathbf{w}}^{L-2})^T A_{L-1}^TA_{L}^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "\\displaystyle B_{L-1}^TA_{L}^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "\\displaystyle B_L^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\n",
    "\\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "We can keep on going until we get to\n",
    "$$\n",
    "\\nabla f(\\overline{\\mathbf{w}}) = \\begin{bmatrix}\n",
    "\\displaystyle \n",
    "B_1^T A_2^T A_3^T\\dotsm A_{L-1}^TA_{L}^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "\\displaystyle \n",
    "B_2^T A_3^T A_4^T\\dotsm A_{L-1}^TA_{L}^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "\\vdots\\\\\n",
    "\\displaystyle B_{L-1}^TA_{L}^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\\\\\n",
    "\\displaystyle B_L^T\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\displaystyle \n",
    "B_1^T A_2^T A_3^T\\dotsm A_{L-1}^TA_{L}^T\\\\\n",
    "\\displaystyle \n",
    "B_2^T A_3^T A_4^T\\dotsm A_{L-1}^TA_{L}^T\\\\\n",
    "\\vdots\\\\\n",
    "\\displaystyle B_{L-1}^TA_{L}^T\\\\\n",
    "\\displaystyle B_L^T\n",
    "\\end{bmatrix}\\nabla \\ell\\big(\\mathcal{O}_L(\\overline{\\mathbf{w}}^L)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df1f39",
   "metadata": {},
   "source": [
    "### Algorithmic Computation\n",
    "\n",
    "Recall that the matrices are constructed by computing the gradient of $\\mathbf{g}_i$ at the location of the input at layer $i$. This starts at layer $1$ and then goes to layer $2$ and so one. But in order to compute the gradient we started with layer $L$ and then plug go backwards to layer $L-1$ until we get the formula above.\n",
    "\n",
    "We can now turn this into an algorithm:\n",
    "\n",
    "* **Initialize** Set $\\mathbf{z}_1 = \\mathbf{x}$.\n",
    "\n",
    "* **For** Layer $i = 1,2,\\dotsm, L$ (forward loop)\n",
    "\n",
    "     * Define $\\mathbf{z}_{i+1} = \\mathbf{g}_i(\\mathbf{z}_i,\\mathbf{w}_i)$ as the output of layer $i$. Note that $\\mathbf{z}_{L+1} = \\mathcal{O}_L(\\overline{\\mathbf{w}}).$\n",
    "     * Define $\\begin{bmatrix} A_{i} & B_i\\end{bmatrix} = J_{\\mathbf{g}_i}(\\mathbf{z}_i, \\mathbf{w}_i)$ as the Jacobian.\n",
    "* Define Loss:\n",
    "    * Define $\\mathbf{z}_{L+2} = \\ell(\\mathbf{z}_{L+1},\\mathbf{y})$\n",
    "    * Define $\\mathbf{p}_{L+1} = \\nabla \\ell(\\mathbf{z}_{L+1},\\mathbf{y})$ where the gradient only includes the gradient w.r.t. the $z$ variables.\n",
    "* **For** Layer $i = L,L-1,\\dotsm, 1$ (backwards loop)\n",
    "    * Define $\\displaystyle \n",
    "\\begin{bmatrix}    \\mathbf{p}_{i}\\\\ \\mathbf{q}_i \\end{bmatrix} = \\begin{bmatrix}\n",
    "A_i^T\\\\B_i^T\n",
    "\\end{bmatrix}\\mathbf{p}_{i+1}\n",
    "    $\n",
    "* **Output**\n",
    "$\\displaystyle \\nabla f(\\overline{\\mathbf{w}}) = \\begin{bmatrix}\n",
    "\\mathbf{q}_1\\\\\\mathbf{q}_2\\\\\\vdots\\\\\\mathbf{q}_L\n",
    "\\end{bmatrix}\\in \\mathbb{R}^{r_1+\\dotsm+r_L}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2946d5ee",
   "metadata": {},
   "source": [
    "## EXAMPLE\n",
    "\n",
    "We pull up an advertising data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9bca69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e86355-0a0a-483e-a90a-f5ea37f92b1b",
   "metadata": {},
   "source": [
    "We will use linear regression, so first we include some basic functions. \n",
    "\n",
    "The first ```mmids_gramschmidt``` is Gram-Schmidt which spits out the $QR$ decomposition of a matrix $A$. \n",
    "\n",
    "Then ```mmids_backsubs``` solves $U\\mathbf{x}= \\mathbf{b}$ where $U$ is an upper-triangular matrix using back-substitution. \n",
    "\n",
    "Finally, ```ls_by_qr``` solves least-squares by using the $QR$ decomposition which we know is\n",
    "$$\n",
    "\\mathbf{x} = R^{-1} Q^T \\mathbf{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d299b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmids_gramschmidt(A):\n",
    "    B = np.copy(A)\n",
    "    (n,m) = B.shape\n",
    "    Q = np.zeros((n,m))\n",
    "    R = np.zeros((m,m))\n",
    "    for j in range(m):\n",
    "        v = B[:,j]\n",
    "        for i in range(j):\n",
    "            R[i,j] = np.dot(Q[:,i], B[:,j])\n",
    "            v -= R[i,j]*Q[:,i]$\n",
    "            \n",
    "        R[j,j] = LA.norm(v)\n",
    "        Q[:,j] = v/R[j,j]\n",
    "    return Q, R\n",
    "def mmids_backsubs(U,b):\n",
    "    m = b.shape[0]\n",
    "    x = np.zeros(m)\n",
    "    for i in reversed(range(m)):\n",
    "        x[i] = (b[i] - np.dot(U[i,i+1:m],x[i+1:m]))/U[i,i]\n",
    "    return x\n",
    "\n",
    "def ls_by_qr(A, b):\n",
    "    Q, R = mmids_gramschmidt(A)\n",
    "    return mmids_backsubs(R, Q.T @ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9491e-7610-4d38-b069-29c82b7a4dfe",
   "metadata": {},
   "source": [
    "Let's look at the first few rows of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7d0045-630b-4636-9cf3-c12ba9f3b043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     TV  radio  newspaper  sales\n",
       "0           1  230.1   37.8       69.2   22.1\n",
       "1           2   44.5   39.3       45.1   10.4\n",
       "2           3   17.2   45.9       69.3    9.3\n",
       "3           4  151.5   41.3       58.5   18.5\n",
       "4           5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('CSV/advertising.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5fa86-828f-45ac-a40c-df34efc7b992",
   "metadata": {},
   "source": [
    "How big is the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ca71e48-2fe5-4691-9e2b-979e9122ccdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "n = len(df.index)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7f241-1d68-426e-ad38-8c7aca640b0c",
   "metadata": {},
   "source": [
    "Let's get the TV advertising, radia advertising, and newspaper advertising as a vectors (they are lists here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ded66b44-3c3f-4e3a-afbf-ec9682e1487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230.1  44.5  17.2 151.5 180.8]\n"
     ]
    }
   ],
   "source": [
    "TV = df['TV'].to_numpy()\n",
    "radio = df['radio'].to_numpy()\n",
    "newspaper = df['newspaper'].to_numpy()\n",
    "sales = df['sales'].to_numpy()\n",
    "print(TV[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910ae30-89ce-491f-a4b7-3779e877054c",
   "metadata": {},
   "source": [
    "Let's now form the matrix $A$ whose columns are the TV, radio and newspaper spending and there is an additional column of all ones. We'll print just the first few rows of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ae49fd-9328-492d-95b6-40eeabe14f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.  230.1  37.8  69.2]\n",
      " [  1.   44.5  39.3  45.1]\n",
      " [  1.   17.2  45.9  69.3]\n",
      " [  1.  151.5  41.3  58.5]\n",
      " [  1.  180.8  10.8  58.4]]\n"
     ]
    }
   ],
   "source": [
    "features = np.stack((TV, radio, newspaper), axis=-1)\n",
    "A = np.c_[np.ones(n), features]\n",
    "print(A[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a655f-7117-4e6c-a89d-b1da50f29d14",
   "metadata": {},
   "source": [
    "Let's now look at the least squares coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b4488eb-25e0-4a76-a756-1a788d6fbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]\n"
     ]
    }
   ],
   "source": [
    "coeff = ls_by_qr(A, sales)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5ad9b-e4b7-4a17-a8bd-9dc62bdd60f1",
   "metadata": {},
   "source": [
    "This means that \n",
    "$$\n",
    "\\text{SALES} \\approx 2.94 + 0.05\\times \\text{TV} + 0.19 \\times\\text{RADIO} + 0.00\\times\\text{PRINT}.\n",
    "$$\n",
    "\n",
    "How good is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55fe16eb-25a6-4ee9-ab91-87384ee2e0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7841263145109365"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((A @ coeff - sales)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e2bb9-80a7-4106-b6bd-4f0e6d35af88",
   "metadata": {},
   "source": [
    "What happens if we use a shallow (i.e. small $L$) multi-layer function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f177d0-1060-4e2d-9e14-d4fd8bf75151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80182d9c-d071-4c32-b673-b48eb1a57536",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((features, sales))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "712d3993-4613-4f78-82a7-75088b26b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ee66bb5-20f5-4c88-ab30-8ab04309f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4 (16.00 Byte)\n",
      "Trainable params: 4 (16.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    " layers.Dense(input_dim=3, units=1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4b56809-f5a6-4a02-9e38-112b9d359ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    " optimizer=tf.optimizers.SGD(learning_rate=1e-5),\n",
    " loss='mean_squared_error'\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09e51fbb-7a9d-450d-a016-fe3788482c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ee45189cc0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    " train_dataset, batch_size=64, epochs=np.int32(1e4), verbose=0\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1facde1b-4329-4b69-a1da-45337a63fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.0548817 ],\n",
      "       [0.21836095],\n",
      "       [0.01579611]], dtype=float32), array([0.30926073], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1334d1b7-274a-4507-9d54-9897c737db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 3.9137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.9137401580810547"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdcdb4-05d6-45f1-9ebe-242cfdcfc98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
